{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course Set-Up\n",
    "\n",
    "Welcome to MIDS W266: Natural Language Processing! \n",
    "\n",
    "This notebook is a quick guide to getting set up with the programming environment we'll be using this semester. We'll be using IPython (Jupyter) notebooks for most of the course exercises and assignments, and we'll make heavy use of NumPy, scikit-learn, TensorFlow, and NLTK. \n",
    "\n",
    "The instructions below should get you set up for most of the semester, although since this is the first time this course is offered we might add a few new things as we go along.\n",
    "\n",
    "We'll do our best to support a variety of platforms, but most NLP software (and Data Science software in general) works best on UNIX-based operating systems, i.e. **Linux** or **Mac OSX**. While you'll be doing most coding in Python, it'll be very useful to be familiar with the **bash** command-line environment and common utilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Instance set-up\n",
    "\n",
    "If you plan on using a GCE instance for the course, follow the instructions on the [Cloud Instance Guide](https://github.com/datasci-w266/main/blob/master/cloud/README.md). This will walk you through cloning the git repo, installing Anaconda and TensorFlow, and setting up to use Jupyter notebooks.\n",
    "\n",
    "When you're done, run a notebook server on your cloud instance:\n",
    "```\n",
    "cd ~\n",
    "jupyter notebook\n",
    "```\n",
    "And in your browser, navigate to http://localhost:8888/notebooks/w266/week1/Course%20Set-Up.ipynb to load a live version of this notebook. (_If you cloned to a different folder than `~/W266`, you might need to change this URL, or browse from the [tree view](http://localhost:8888/tree)_)\n",
    "\n",
    "You can skip the local set-up section and jump down to the **NLTK** part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local set-up\n",
    "\n",
    "If you plan on working on your local machine, first install `git` and clone the course repo:\n",
    "```\n",
    "git clone https://github.com/datasci-w266/main.git ~/w266\n",
    "```\n",
    "Or if you have authentication issues:\n",
    "```\n",
    "git clone git@github.com:datasci-w266/main.git ~/w266\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local set-up: Python\n",
    "\n",
    "We strongly recommend the [**Anaconda**](https://www.continuum.io/downloads) python distribution, which includes NumPy, scikit-learn, matplotlib, pandas, NLTK, and many other useful packages. For most code, we'll assume that you have Anaconda, and mention explicitly anything else that's not included.\n",
    "\n",
    "Download the Python 2.7 version from https://www.continuum.io/downloads, and follow the instructions to install.\n",
    "\n",
    "There are a few Python features that we'll be making use of that you might not have encounted in previous courses. You might want to bookmark these, and take a glance at the documentation for these now - although we'll explain them more as the appear.\n",
    "\n",
    "- [Generators and generator expressions](https://wiki.python.org/moin/Generators), handy for working with streams of text\n",
    "- [SciPy sparse matricies](http://docs.scipy.org/doc/scipy/reference/sparse.html) for representing large \"one hot\" vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local set-up: TensorFlow\n",
    "\n",
    "[TensorFlow](https://www.tensorflow.org/) is Google's open-source numerical computation library. It's designed for deep learning, and has very good support for the neural network architectures, such as RNNs, that are commonly used in NLP.\n",
    "\n",
    "**Note:** TensorFlow is only available for Linux and OSX. If you're on Windows, the easiest option is to use a Google Cloud instance. See the [Cloud Instance Guide](https://github.com/datasci-w266/main/cloud/README.md) for more details.\n",
    "\n",
    "If you're using Anaconda as above, you can install the latest version of TensorFlow with:\n",
    "```\n",
    "conda install -c jjhelmus tensorflow\n",
    "```\n",
    "\n",
    "Alternatively, you can follow the Pip Installation instructions here: https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#pip-installation (Ignore everything about conda or virtualenv environments.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced: TensorFlow and GPUs\n",
    "\n",
    "TensorFlow can use a GPU to dramatically accelerate running neural network models. If you have a recent NVidia GPU, follow the instructions here to get it set up:\n",
    "https://www.tensorflow.org/versions/r0.10/get_started/os_setup.html#optional-linux-enable-gpu-support\n",
    "\n",
    "Be warned that CUDA and NVidia drivers on Linux can be finicky and sometimes unstable, so consult the course staff if you plan on going this route."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Local set-up: Notebooks\n",
    "\n",
    "If the above steps completed successfully, you should be able to open a notebook with:\n",
    "```\n",
    "cd ~\n",
    "jupyter notebook &\n",
    "```\n",
    "It should open a browser window to http://localhost:8888/tree; find this notebook and open it to continue. You can also try the direct link:\n",
    "- http://localhost:8888/notebooks/w266/week1/Course%20Set-Up.ipynb\n",
    "\n",
    "Run the cells below to test your Python installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n",
      "Welcome to Natural Language Processing!\n"
     ]
    }
   ],
   "source": [
    "print \"Hello world!\"\n",
    "print \"Welcome to Natural Language Processing!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, TensorFlow!\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "hello = tf.constant(\"Hello, TensorFlow!\")\n",
    "sess = tf.Session()\n",
    "print sess.run(hello)\n",
    "\n",
    "a = tf.constant(10)\n",
    "b = tf.constant(32)\n",
    "print sess.run(a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "Initialized!\n",
      "Step 0 (epoch 0.00), 11.6 ms\n",
      "Minibatch loss: 12.053, learning rate: 0.010000\n",
      "Minibatch error: 90.6%\n",
      "Validation error: 84.6%\n",
      "Step 100 (epoch 0.12), 490.5 ms\n",
      "Minibatch loss: 3.279, learning rate: 0.010000\n",
      "Minibatch error: 6.2%\n",
      "Validation error: 7.1%\n",
      "Step 200 (epoch 0.23), 501.0 ms\n",
      "Minibatch loss: 3.503, learning rate: 0.010000\n",
      "Minibatch error: 12.5%\n",
      "Validation error: 3.6%\n",
      "Step 300 (epoch 0.35), 498.5 ms\n",
      "Minibatch loss: 3.199, learning rate: 0.010000\n",
      "Minibatch error: 7.8%\n",
      "Validation error: 3.4%\n",
      "Step 400 (epoch 0.47), 502.7 ms\n",
      "Minibatch loss: 3.239, learning rate: 0.010000\n",
      "Minibatch error: 10.9%\n",
      "Validation error: 2.6%\n",
      "Step 500 (epoch 0.58), 500.3 ms\n",
      "Minibatch loss: 3.283, learning rate: 0.010000\n",
      "Minibatch error: 9.4%\n",
      "Validation error: 2.6%\n",
      "Step 600 (epoch 0.70), 898.0 ms\n",
      "Minibatch loss: 3.212, learning rate: 0.010000\n",
      "Minibatch error: 7.8%\n",
      "Validation error: 2.8%\n",
      "Step 700 (epoch 0.81), 972.4 ms\n",
      "Minibatch loss: 3.006, learning rate: 0.010000\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 2.5%\n",
      "Step 800 (epoch 0.93), 798.0 ms\n",
      "Minibatch loss: 3.090, learning rate: 0.010000\n",
      "Minibatch error: 6.2%\n",
      "Validation error: 2.2%\n",
      "Step 900 (epoch 1.05), 490.1 ms\n",
      "Minibatch loss: 2.926, learning rate: 0.009500\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.7%\n",
      "Step 1000 (epoch 1.16), 489.8 ms\n",
      "Minibatch loss: 2.857, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.7%\n",
      "Step 1100 (epoch 1.28), 492.7 ms\n",
      "Minibatch loss: 2.823, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.6%\n",
      "Step 1200 (epoch 1.40), 538.4 ms\n",
      "Minibatch loss: 2.935, learning rate: 0.009500\n",
      "Minibatch error: 7.8%\n",
      "Validation error: 1.5%\n",
      "Step 1300 (epoch 1.51), 500.3 ms\n",
      "Minibatch loss: 2.760, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.7%\n",
      "Step 1400 (epoch 1.63), 622.0 ms\n",
      "Minibatch loss: 2.795, learning rate: 0.009500\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 1.5%\n",
      "Step 1500 (epoch 1.75), 547.3 ms\n",
      "Minibatch loss: 2.875, learning rate: 0.009500\n",
      "Minibatch error: 6.2%\n",
      "Validation error: 1.3%\n",
      "Step 1600 (epoch 1.86), 520.5 ms\n",
      "Minibatch loss: 2.700, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.3%\n",
      "Step 1700 (epoch 1.98), 511.5 ms\n",
      "Minibatch loss: 2.650, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.4%\n",
      "Step 1800 (epoch 2.09), 516.6 ms\n",
      "Minibatch loss: 2.654, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.3%\n",
      "Step 1900 (epoch 2.21), 515.4 ms\n",
      "Minibatch loss: 2.674, learning rate: 0.009025\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.2%\n",
      "Step 2000 (epoch 2.33), 502.4 ms\n",
      "Minibatch loss: 2.638, learning rate: 0.009025\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.2%\n",
      "Step 2100 (epoch 2.44), 503.1 ms\n",
      "Minibatch loss: 2.585, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.1%\n",
      "Step 2200 (epoch 2.56), 537.0 ms\n",
      "Minibatch loss: 2.576, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.1%\n",
      "Step 2300 (epoch 2.68), 512.4 ms\n",
      "Minibatch loss: 2.551, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.1%\n",
      "Step 2400 (epoch 2.79), 506.7 ms\n",
      "Minibatch loss: 2.508, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 2500 (epoch 2.91), 493.4 ms\n",
      "Minibatch loss: 2.486, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 2600 (epoch 3.03), 501.3 ms\n",
      "Minibatch loss: 2.454, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 2700 (epoch 3.14), 497.9 ms\n",
      "Minibatch loss: 2.508, learning rate: 0.008574\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 1.2%\n",
      "Step 2800 (epoch 3.26), 490.6 ms\n",
      "Minibatch loss: 2.421, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 2900 (epoch 3.37), 494.0 ms\n",
      "Minibatch loss: 2.451, learning rate: 0.008574\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.2%\n",
      "Step 3000 (epoch 3.49), 488.7 ms\n",
      "Minibatch loss: 2.396, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 3100 (epoch 3.61), 500.9 ms\n",
      "Minibatch loss: 2.400, learning rate: 0.008574\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.0%\n",
      "Step 3200 (epoch 3.72), 489.9 ms\n",
      "Minibatch loss: 2.336, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 3300 (epoch 3.84), 489.5 ms\n",
      "Minibatch loss: 2.322, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 3400 (epoch 3.96), 489.5 ms\n",
      "Minibatch loss: 2.302, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 3500 (epoch 4.07), 512.8 ms\n",
      "Minibatch loss: 2.275, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 3600 (epoch 4.19), 494.5 ms\n",
      "Minibatch loss: 2.256, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 3700 (epoch 4.31), 501.2 ms\n",
      "Minibatch loss: 2.236, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 3800 (epoch 4.42), 488.6 ms\n",
      "Minibatch loss: 2.224, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 3900 (epoch 4.54), 488.2 ms\n",
      "Minibatch loss: 2.314, learning rate: 0.008145\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 0.9%\n",
      "Step 4000 (epoch 4.65), 485.5 ms\n",
      "Minibatch loss: 2.223, learning rate: 0.008145\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.1%\n",
      "Step 4100 (epoch 4.77), 496.5 ms\n",
      "Minibatch loss: 2.176, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 4200 (epoch 4.89), 494.3 ms\n",
      "Minibatch loss: 2.217, learning rate: 0.008145\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.1%\n",
      "Step 4300 (epoch 5.00), 491.8 ms\n",
      "Minibatch loss: 2.207, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 4400 (epoch 5.12), 494.3 ms\n",
      "Minibatch loss: 2.149, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.1%\n",
      "Step 4500 (epoch 5.24), 499.4 ms\n",
      "Minibatch loss: 2.178, learning rate: 0.007738\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 0.9%\n",
      "Step 4600 (epoch 5.35), 492.1 ms\n",
      "Minibatch loss: 2.089, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 4700 (epoch 5.47), 497.6 ms\n",
      "Minibatch loss: 2.082, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 4800 (epoch 5.59), 485.7 ms\n",
      "Minibatch loss: 2.055, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 4900 (epoch 5.70), 489.9 ms\n",
      "Minibatch loss: 2.049, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 5000 (epoch 5.82), 488.6 ms\n",
      "Minibatch loss: 2.124, learning rate: 0.007738\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.0%\n",
      "Step 5100 (epoch 5.93), 511.1 ms\n",
      "Minibatch loss: 2.014, learning rate: 0.007738\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 5200 (epoch 6.05), 492.2 ms\n",
      "Minibatch loss: 2.055, learning rate: 0.007351\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 0.9%\n",
      "Step 5300 (epoch 6.17), 613.4 ms\n",
      "Minibatch loss: 1.980, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 5400 (epoch 6.28), 537.6 ms\n",
      "Minibatch loss: 1.956, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 5500 (epoch 6.40), 547.4 ms\n",
      "Minibatch loss: 1.956, learning rate: 0.007351\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 5600 (epoch 6.52), 514.8 ms\n",
      "Minibatch loss: 1.926, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 5700 (epoch 6.63), 514.1 ms\n",
      "Minibatch loss: 1.914, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 5800 (epoch 6.75), 501.0 ms\n",
      "Minibatch loss: 1.897, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.7%\n",
      "Step 5900 (epoch 6.87), 496.9 ms\n",
      "Minibatch loss: 1.887, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 6000 (epoch 6.98), 490.8 ms\n",
      "Minibatch loss: 1.913, learning rate: 0.007351\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 6100 (epoch 7.10), 513.0 ms\n",
      "Minibatch loss: 1.858, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6200 (epoch 7.21), 484.6 ms\n",
      "Minibatch loss: 1.843, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 6300 (epoch 7.33), 483.3 ms\n",
      "Minibatch loss: 1.841, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6400 (epoch 7.45), 484.5 ms\n",
      "Minibatch loss: 1.893, learning rate: 0.006983\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 0.9%\n",
      "Step 6500 (epoch 7.56), 482.0 ms\n",
      "Minibatch loss: 1.808, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6600 (epoch 7.68), 484.7 ms\n",
      "Minibatch loss: 1.848, learning rate: 0.006983\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.0%\n",
      "Step 6700 (epoch 7.80), 482.4 ms\n",
      "Minibatch loss: 1.781, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6800 (epoch 7.91), 483.5 ms\n",
      "Minibatch loss: 1.772, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 6900 (epoch 8.03), 483.7 ms\n",
      "Minibatch loss: 1.762, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 7000 (epoch 8.15), 482.7 ms\n",
      "Minibatch loss: 1.768, learning rate: 0.006634\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.0%\n",
      "Step 7100 (epoch 8.26), 483.0 ms\n",
      "Minibatch loss: 1.740, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7200 (epoch 8.38), 489.9 ms\n",
      "Minibatch loss: 1.746, learning rate: 0.006634\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 7300 (epoch 8.49), 488.2 ms\n",
      "Minibatch loss: 1.722, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7400 (epoch 8.61), 484.6 ms\n",
      "Minibatch loss: 1.700, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 7500 (epoch 8.73), 482.7 ms\n",
      "Minibatch loss: 1.693, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 7600 (epoch 8.84), 487.6 ms\n",
      "Minibatch loss: 1.825, learning rate: 0.006634\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.8%\n",
      "Step 7700 (epoch 8.96), 482.0 ms\n",
      "Minibatch loss: 1.666, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7800 (epoch 9.08), 491.0 ms\n",
      "Minibatch loss: 1.661, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 7900 (epoch 9.19), 484.5 ms\n",
      "Minibatch loss: 1.651, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 8000 (epoch 9.31), 483.5 ms\n",
      "Minibatch loss: 1.661, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 8100 (epoch 9.43), 481.9 ms\n",
      "Minibatch loss: 1.627, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 8200 (epoch 9.54), 485.6 ms\n",
      "Minibatch loss: 1.624, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 8300 (epoch 9.66), 481.4 ms\n",
      "Minibatch loss: 1.611, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 8400 (epoch 9.77), 483.0 ms\n",
      "Minibatch loss: 1.596, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 8500 (epoch 9.89), 483.2 ms\n",
      "Minibatch loss: 1.601, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Test error: 0.8%\n"
     ]
    }
   ],
   "source": [
    "# This is the same as calling python -m (...) on the command line\n",
    "# You should see a bunch of output, and a final test error around 0.8%\n",
    "# It might take a few minutes on a slower machine.\n",
    "%run -m tensorflow.models.image.mnist.convolutional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll interact with TensorFlow as a Python library, but it's really a whole programming system in itself. Continue on to the [**TensorFlow Tutorial Notebook**](TensorFlow%20Tutorial.ipynb) to learn more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "[NLTK](http://www.nltk.org/) is a large compilation of Python NLP packages. It includes implementations of a number of classic NLP models, as well as utilities for working with linguistic data structures, preprocessing text, and managing corpora.\n",
    "\n",
    "NLTK is included with Anaconda, but the corpora need to be downloaded separately. Be warned that this will take up around 3.2 GB of disk space if you download everything! If this is too much, you can download individual corpora as you need them through the same interface.\n",
    "\n",
    "Type the following into a Python shell. It'll open a pop-up UI with the downloader:\n",
    "```\n",
    "import nltk\n",
    "nltk.download()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can explore the corpora a bit. Let's look at the famous [Brown corpus](http://www.essex.ac.uk/linguistics/external/clmt/w3c/corpus_ling/content/corpora/list/private/brown/brown.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pip install --upgrade pip\n",
    "#pip install --upgrade nltk\n",
    "import nltk\n",
    "#nltk.download()\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Look at the first five sentences\n",
    "for s in brown.sents()[:5]:\n",
    "    print \" \".join(s)\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# As words\n",
    "print \"\\n\".join(brown.words()[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK also includes a sample of the [Penn treebank](https://www.cis.upenn.edu/~treebank/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "# Look at the first five sentences\n",
    "for s in treebank.sents()[:5]:\n",
    "    print \" \".join(s)\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at the parse of a sentence.\n",
    "# Don't worry about what this means yet!\n",
    "idx = 45\n",
    "print \" \".join(treebank.sents()[idx])\n",
    "print \"\"\n",
    "print treebank.parsed_sents()[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the [Europarl corpus](http://www.statmt.org/europarl/), which consists of *parallel* text - a sentence and its translations to multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import europarl_raw\n",
    "\n",
    "idx = 0\n",
    "\n",
    "print \"ENGLISH: \" + \" \".join(europarl_raw.english.sents()[idx])\n",
    "print \"\"\n",
    "print \"FRENCH: \" + \" \".join(europarl_raw.french.sents()[idx])\n",
    "print \"\"\n",
    "print \"SPANISH: \" + \" \".join(europarl_raw.spanish.sents()[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
